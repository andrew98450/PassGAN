{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.functional import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PasswordDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root = \"train_lite.txt\", seq_len = 20) -> None:\n",
    "        super(PasswordDataset, self).__init__()\n",
    "        self.root = root\n",
    "        self.seq_len = seq_len\n",
    "        self.vocabs = build_vocab_from_iterator(self.get_vocab(), specials=[\"<start>\", \"'\"], max_tokens=100)\n",
    "        self.stoi = self.vocabs.get_stoi()\n",
    "        self.itos = self.vocabs.get_itos()\n",
    "        self.inputs = self.get_inputs()\n",
    "        json.dump({\"stoi\": self.stoi, \"itos\": self.itos}, open(\"password_charmap.json\", \"w\"))\n",
    "\n",
    "    def get_vocab(self):\n",
    "        fd = open(self.root, \"r\")\n",
    "        for text in fd:\n",
    "            yield [chars for chars in str(text).strip()]\n",
    "        fd.close()\n",
    "\n",
    "    def get_inputs(self):\n",
    "        fd = open(self.root, \"r\")\n",
    "        inputs = []\n",
    "        for text in fd:\n",
    "            text_data = [self.stoi[chars] for chars in str(text).strip()]\n",
    "            text_data.insert(0, self.stoi[\"<start>\"])\n",
    "            while len(text_data) < self.seq_len:\n",
    "                text_data.append(self.stoi[\"'\"])\n",
    "            inputs.append(text_data[:self.seq_len])\n",
    "        inputs = numpy.array(inputs, dtype=numpy.int32)\n",
    "        fd.close()\n",
    "        return inputs\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.inputs[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(torch.nn.Module):\n",
    " \n",
    "    def __init__(self, in_channel): \n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv_layer = torch.nn.Sequential(\n",
    "            torch.nn.LeakyReLU(0.02),\n",
    "            torch.nn.Conv1d(in_channel, in_channel, 3, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm1d(in_channel),\n",
    "            torch.nn.LeakyReLU(0.02),\n",
    "            torch.nn.Conv1d(in_channel, in_channel, 3, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm1d(in_channel))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        outputs = self.conv_layer(inputs)\n",
    "        return (outputs * 0.3) + inputs\n",
    "    \n",
    "class NetG(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, seq_len, vocab_len):\n",
    "        super(NetG, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.fc_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 512, bias=False),\n",
    "            torch.nn.LeakyReLU(0.02),\n",
    "            torch.nn.Linear(512, 128 * seq_len, bias=False))\n",
    "        \n",
    "        self.conv_layer = torch.nn.Sequential(\n",
    "            ResBlock(128),\n",
    "            ResBlock(128), \n",
    "            ResBlock(128),\n",
    "            ResBlock(128),             \n",
    "            ResBlock(128),\n",
    "            torch.nn.Conv1d(128, vocab_len, 3, padding=1, bias=False),\n",
    "            torch.nn.Softmax(1))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.fc_layer(inputs)\n",
    "        outputs = outputs.reshape(-1, 128, self.seq_len)\n",
    "        outputs = self.conv_layer(outputs)\n",
    "        outputs = outputs.permute(0, 2, 1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetD(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, seq_len, vocab_len):\n",
    "        super(NetD, self).__init__()\n",
    "        self.conv_layer = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(vocab_len, 128, 3, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm1d(128),\n",
    "            torch.nn.ReLU(),\n",
    "            ResBlock(128),\n",
    "            ResBlock(128), \n",
    "            ResBlock(128),\n",
    "            ResBlock(128),             \n",
    "            ResBlock(128))\n",
    "        \n",
    "        self.fc_layer = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(128 * seq_len, 512, bias=False),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 1, bias=False))        \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.permute(0, 2, 1)\n",
    "        outputs = self.conv_layer(inputs)\n",
    "        outputs = self.fc_layer(outputs)\n",
    "        return outputs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "lr = 0.0001\n",
    "batch_size = 64\n",
    "seq_len = 20\n",
    "c = 0.01\n",
    "n_c = 5\n",
    "\n",
    "dataset = PasswordDataset(seq_len=seq_len)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "vocab_size = len(dataset.stoi)\n",
    "print(len(dataset))\n",
    "print(vocab_size)\n",
    "\n",
    "G = NetG(seq_len, vocab_size)\n",
    "D = NetD(seq_len, vocab_size)\n",
    "\n",
    "G = G.cuda()\n",
    "D = D.cuda()\n",
    "\n",
    "g_optim = torch.optim.Adam(params=G.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "d_optim = torch.optim.Adam(params=D.parameters(), lr=lr, betas=(0.5, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for iters, inputs in enumerate(dataloader, 0):\n",
    "        \n",
    "        real_data = inputs.long().cuda()\n",
    "        fake_data = torch.randn(batch_size, 128).float().cuda()\n",
    "        \n",
    "        real_data = one_hot(real_data, vocab_size).float()\n",
    "        \n",
    "        for _ in range(n_c):\n",
    "            with torch.no_grad():\n",
    "                generate_data = G(fake_data)\n",
    "            d_real = D(real_data)\n",
    "            d_fake = D(generate_data)  \n",
    "            d_real_loss = -torch.mean(d_real)\n",
    "            d_fake_loss = torch.mean(d_fake)\n",
    "            d_loss = d_fake_loss + d_real_loss\n",
    "            \n",
    "            d_optim.zero_grad()\n",
    "            d_loss.backward()\n",
    "            d_optim.step()\n",
    "            \n",
    "            for p in D.parameters():\n",
    "                p.data = p.data.clamp(-c, c)\n",
    "        \n",
    "        fake_data = torch.randn(batch_size, 128).float().cuda()   \n",
    "        generate_data = G(fake_data)\n",
    "        d_fake = D(generate_data)\n",
    "        g_loss = -torch.mean(d_fake)\n",
    "        \n",
    "        g_optim.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optim.step()\n",
    "        \n",
    "        if iters % 10 == 0:\n",
    "            print(\"[+] Epoch: [%d/%d] G_Loss: %.4f D_Loss: %.4f\" % (epoch+1, epochs, g_loss, d_loss))\n",
    "            with torch.no_grad():\n",
    "                sample_size = 5\n",
    "                fake_data = torch.randn(sample_size, 128).float().cuda()  \n",
    "                generate_data = G(fake_data) \n",
    "                generate_data = torch.distributions.Categorical(probs=generate_data)\n",
    "                generate_data = generate_data.sample()\n",
    "                text = ''\n",
    "                for i in range(sample_size):\n",
    "                    for j in range(seq_len):\n",
    "                        text += dataset.itos[generate_data[i, j]]\n",
    "                    text += \"\\n\"\n",
    "                print(text.rstrip(\"\\n\"))\n",
    "\n",
    "G = G.cpu()\n",
    "D = D.cpu()\n",
    "\n",
    "torch.save(G.state_dict(), \"passgan_modelG.pth\")\n",
    "torch.save(D.state_dict(), \"passgan_modelD.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = open(f\"gen_passgan_password.txt\", \"w\", encoding='utf-8')\n",
    "itos = json.load(open(\"./password_charmap.json\", \"r\"))['itos']\n",
    "model = NetG(seq_len, vocab_size)\n",
    "model.load_state_dict(torch.load(\"./passgan_modelG.pth\"))\n",
    "model.eval()\n",
    "\n",
    "sample_size = 100000\n",
    "batch_size = 200\n",
    "current_size = 0\n",
    "seq_len = 20\n",
    "\n",
    "while current_size < sample_size:\n",
    "    fake_data = torch.randn(batch_size, 128).float() \n",
    "    generate_data = G(fake_data) \n",
    "    generate_data = torch.distributions.Categorical(probs=generate_data)\n",
    "    generate_data = generate_data.sample()\n",
    "    for i in range(batch_size):\n",
    "        text = \"\"\n",
    "        for j in range(seq_len):\n",
    "            if itos[generate_data[i, j]] == \"'\":\n",
    "                break\n",
    "            text += itos[generate_data[i, j]]\n",
    "        fd.write(text + \"\\n\")\n",
    "    current_size += batch_size\n",
    "    print(\"[+] Generate Data: %d\" % current_size)\n",
    "fd.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
